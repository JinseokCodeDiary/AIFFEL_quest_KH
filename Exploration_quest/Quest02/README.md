# Exploration - Quest02

## 코드

- 김기홍

  - [프로젝트] 뉴스 요약 모델 코드

- 김주현

  - 아마존 리뷰용 코드(LMS에 있는 코드에 주석을 달고 목차를 정리한 버전)
    - 코랩 주소 : https://colab.research.google.com/drive/1R-4q2LYQqrbzBy_2ZBNhez7wtwzeKl6B?usp=sharing
  - [프로젝트] 뉴스 요약 모델 코드
    - 코랩 주소 : https://colab.research.google.com/drive/1a8KftU3Uc0qU8ngNz3t8JnvMQbQJyWsB?usp=sharing

## 회고

김기홍

- 데이터의 전처리부터, 할 일도 많고 이해가 느려서 고전했었는데, 영어, 한글 마다 전처리 프로세스가 다르고 감정을 분석하거나 요약하는 목적에 따라서도 전처리가 다르다는 것이 흥미로웠습니다. 그리고 모델 아키텍처 또한 달라서 이해하는데 시간이 오래걸렸음.
- 추가적인 시도로 양방향 LSTM 을 사용해보려고 했는데 어텐션을 추가하고, 예측할 때 디코드부분이 생각처럼 잘 안되어 시간이 오래걸려 제대로 완료하지는 못했고,T5 pretrained 모델을 사용해보았는데, 학습하지 않았는데도 불구하고 생각보다 성능이 좋았다. 이번 회차를 통해 NLP의 기본적인 부분의 이해를 강화해야겠다고 생각했습니다.

김주현

- 트랜스포머가 등장하기 전 가장 중요했던 seq2seq with attention 모델을 직접 구현하고 테스트해볼 수 있어서 흥미로웠습니다.
- 사전 학습된 모델도 여러개 가져와 비교해 보고 싶었으나, 이번에는 Facebook의 BART 모델을 한번 사용해본 것으로 만족해야겠습니다.
- 기홍님과 함께 하면서 기홍님이 시도하시는 여러 시도로부터 좋은 자극을 받았습니다. 특히, 양방향 LSTM을 적용해 보는 시도가 인상 깊었습니다. 또한, 피어 리뷰 시간을 통해 은재님이 인코더 블럭에 LSTM을 1개 층 더 연결하면 어떤 결과가 나올지 실험해 본 것 역시 인상 깊었습니다.
